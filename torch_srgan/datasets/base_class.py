"""
Base class for image pair datasets defined for test, train and validate models.
"""

__author__ = "Marc Bermejo"


import functools
import os
import cv2
import albumentations as A
import torch

from abc import ABC
from typing import Dict, List, Tuple, Optional, Union, Callable
from torch.utils.data import Dataset
from albumentations.pytorch import ToTensorV2
from albumentations.augmentations.crops import functional as cr_func

from .transforms import PairedRandomCrop, PairedCenterCrop, SimpleNormalize
from ..utils.path_iter import images_in_dir


class ImagePairDataset(Dataset, ABC):
    """
    Image pair dataset base class used by all the datasets defined to train, test and validate SR models.

    Args:
        scale_factor: Scale factor relation between the low resolution and the high resolution images.
        train: Flag to indicate if we are on train or validation mode.
            This will affect to the paired crop transform. If not set, a paired random crop will be applied.
            If set, a paired center crop will be applied instead.
        patch_size: High-resolution image crop size. Needs to be a tuple of (H, W).
            If set to None, any crop transform will be applied.
        base_dir: Base directory where datasets are stored.
        hr_img_dir: Directory where high-resolution images are stored.
        lr_img_dir: Directory where low-resolution images are stored. If set to None,
            paired low-resolution images will be generated by scaling down high-resolution images
            by the defined scale factor using bicubic interpolation.
        transforms: List of user-defined transformations to be applied to the dataset images.
    """

    # Define high-resolution (ground truth) image key to be used during transformation pipeline
    _hr_img_transform_key: str = "gt_image"
    # Partially define composed transform to be applied to both images
    PairedCompose: Callable[..., A.Compose] = functools.partial(
        A.Compose, additional_targets={_hr_img_transform_key: "image"}
    )

    def __init__(self, scale_factor: int, train: bool, patch_size: Optional[Tuple[int, int]] = (128, 128),
                 base_dir: str = "data", hr_img_dir: str = "./", lr_img_dir: str = None,
                 transforms: List[A.BasicTransform] = None):
        # Define class member variables from input parameters
        self.scale_factor = scale_factor
        self.train_mode = train
        self.base_dir = base_dir
        self.hr_img_dir = os.path.join(base_dir, hr_img_dir)
        self.img_list: List[Dict[str, str]] = []
        self.patch_size: Optional[Tuple[int, int]] = None
        self.transform: A.Compose = A.Compose([])
        # Initialize transform pipeline
        self.set_dataset_transforms(patch_size, transforms)

        # Check if user provided directories for HR and LR images
        if lr_img_dir:
            self.lr_img_dir = os.path.join(base_dir, lr_img_dir)
        else:
            self.lr_img_dir = None

        # Initialize image pair data dictionary
        self.data: List[Dict[str, str]] = []
        # Set image paths from given folder(s)
        # Retrieve HR images from configured directory
        hr_images = images_in_dir(self.hr_img_dir)
        # Populate data array with HR images
        self.data = [{"hr": image} for image in hr_images]
        # If LR image directory is given, read also paired images from there
        if self.lr_img_dir:
            # Retrieve LR images from configured directory
            lr_images = images_in_dir(self.lr_img_dir)
            # Check that LR images array size matches HR size
            assert len(hr_images) == len(lr_images)
            # Populate data array with HR images
            for i in range(len(lr_images)):
                self.data[i].update({"lr": lr_images[i]})

    def set_dataset_transforms(self, patch_size: Optional[Tuple[int, int]], transforms: List[A.BasicTransform],
                               apply_to_gt_img: bool = False):
        """
        Set the transformations to be applied to the dataset images.

        Args:
            patch_size: High-resolution image crop size. Needs to be a tuple of (H, W).
                If set to None, any crop transform will be applied.
            transforms: List of user-defined transformations to be applied to the dataset images.
            apply_to_gt_img: Flag to indicate if the user-defined transformations need to be applied to both
                low and high resolution images (if `True`) or only to the low resolution image.
        """
        # Initialize transform pipeline
        transform_pipeline = []
        # Define pre-processing transform depending on if it is a train dataset or not and if a patch size was given
        if patch_size:
            if self.train_mode:
                # Perform a random crop to both HR and LR images during training
                paired_crop = PairedRandomCrop(
                    patch_size, paired_img_scale=self.scale_factor,
                    scaled_img_target_key=self._hr_img_transform_key, always_apply=True
                )
            else:
                # Perform a center crop to both HR and LR images during test/validation
                paired_crop = PairedCenterCrop(
                    patch_size, paired_img_scale=self.scale_factor,
                    scaled_img_target_key=self._hr_img_transform_key, always_apply=True
                )
            # Append to full pipeline
            transform_pipeline.append(paired_crop)
            # Update patch size defined in class
            self.patch_size = patch_size
        # Define user requested transformation pipeline (if any)
        if transforms:
            compose_class = self.PairedCompose if apply_to_gt_img else A.Compose
            user_transforms = compose_class(transforms)
            # Append to full pipeline
            transform_pipeline.append(user_transforms)
        # Define post-processing transforms (normalize image and convert it to Tensor)
        post_transforms = self.PairedCompose(
            [SimpleNormalize(), ToTensorV2()],
            additional_targets={self._hr_img_transform_key: "image"}
        )
        # Append to full pipeline
        transform_pipeline.append(post_transforms)

        # Define complete transformation pipeline
        self.transform = A.Compose(transform_pipeline)

    def __getitem__(self, index: int) -> \
            Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor, torch.Tensor, dict]]:
        """
        Retrieve a paired low and high resolution image from the dataset.

        Args:
            index: Dataset paired image index.

        Returns:
            Tuple of (low resolution image, high resolution image)
        """
        # Retrieve image pair from data array
        img_pair = self.data[index]
        # Read HR image from disk
        hr_image = cv2.imread(img_pair["hr"], cv2.IMREAD_UNCHANGED)
        # Check if LR image was found in dataset
        if not img_pair.get("lr", None):
            # Crop HR image first to ensure that its size is multiple of scale
            hr_img_h, hr_img_w = hr_image.shape[:2]
            hr_img_h -= hr_img_h % self.scale_factor
            hr_img_w -= hr_img_w % self.scale_factor
            hr_image = cr_func.center_crop(hr_image, hr_img_h, hr_img_w)
            # Calculate LR image resize scaling factor
            lr_img_size = (hr_img_w // self.scale_factor, hr_img_h // self.scale_factor)
            # Resize HR image to produce an LR image
            lr_image = cv2.resize(hr_image, lr_img_size, interpolation=cv2.INTER_CUBIC)
        else:
            # Read LR image from disk
            lr_image = cv2.imread(img_pair["lr"], cv2.IMREAD_UNCHANGED)

        # Convert image color space to RGB
        lr_image = cv2.cvtColor(lr_image, cv2.COLOR_BGR2RGB)
        hr_image = cv2.cvtColor(hr_image, cv2.COLOR_BGR2RGB)

        # Apply transformation pipeline to both images
        transformed = self.transform(**{"image": lr_image, self._hr_img_transform_key: hr_image})

        return transformed["image"], transformed[self._hr_img_transform_key]

    def __len__(self) -> int:
        """
        Get length of the dataset.

        Returns:
            Length of the dataset.
        """
        return len(self.data)
